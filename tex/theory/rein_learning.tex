
\section{Reinforcement Learning}
\paragraph{Why reinforcement learning?}
As you might recall from \autoref{chap:intro}, the recurrent visual attention model
extracting information from a picture by attending to certain locations of the picture
and aggregating the information from these locations. This property will make our network
avoid locations with insignificant information, hence ignore locations with clutter.
In order to teach the network
outputs next location to attend given previous location, we need to provide training data to
the neural network. The problem is, that we don't know the right answer for this.
We can only say whether the network made a right classification decision after the network
has already chosen several locations. Consequently, the training of the network parameters will
be a very difficult task. As previously mentioned in \autoref{par:grad_desc}, using gradient descent
with backpropagation for training NN is possible only with differentiable cost function like
mean squared error function or cross entropy function. However, we can't use these functions without
knowing the right answer, therefore defining the cost function would be a complicated task.
This sort of tasks is studied by a field of machine learning called \emph{reinforcement
learning(\gls{RL})}. The theory described in this section based on the
\cite{Sutton2012}, unless otherwise stated.

% \subsection{}
\paragraph{What is reinforcement learning?}
Reinforcement learning concerns with teaching an agent to take actions based on
reward signal, even if this signal is delayed.
These agents are trained to maximise the total
sum of such reward signals.
The underlying idea behind \gls{RL} is to
represent the nature of learning where agent learning about the the world by interacting
with it. By performing these interactions we're observing the changes in the world
from which we can learn about the consequences of these
interactions, and about what interactions to perform to achieve the goal.
Reinforcement learning provides a computational approach to perform goal-directed learning
by interacting with environment. The main difference between supervised learning and
reinforcement learning is that in \gls{RL} there is no instructions about the right answer.
Instead, training information or reward signal is used to evaluate the taken actions.
That is, rather than providing true label for the system instantaneously,
system is receiving a reward signal
after each performed action and the goal of \gls{RL} system is to teach an agent using its own
experience to achieve a certain goal.

% The difference between


% The most important feature distinguishing reinforcement learning from other
% types of learning is that it uses training information that evaluates the
% actions taken rather than instructs by giving correct actions
%
%
%
% The idea that we learn by interacting with our environment is probably the
% first to occur to us when we think about the nature of learning. When an
% infant plays, waves its arms, or looks about, it has no explicit teacher,
% but it does have a direct sensorimotor connection to its environment.
% Exercising this connection produces a wealth of information about and effect, about the consequences of actions, and about what to do in
% order to achieve goals. Throughout our lives, such interactions are
% undoubtedly a major source of knowledge about our environment and ourselves.
% Whether we are learning to drive a car or to hold a conversation, we are
% acutely aware of how our environ- ment responds to what we do, and we seek
% to influence what happens through our behavior. Learning from interaction is
% a foundational idea underlying nearly all theories of learning and
% intelligence.


% used the idea
% need to decide next location based on the predecessor location \
% why do we need it our work?


\subsection{Components of reinforcement learning}
To better understand the main components of RL let's take a
look at one of the recent RL systems where the system
needed to learn how to play Atari 2600 games.

\begin{figure}[H]
	\includegraphics[width=\linewidth,keepaspectratio]{atari_rl.png}
	\caption{
		RL system to play Atari games (source: \cite{mnih2013playing})
		}
	\label{img:atari_rl}
\end{figure}
% The rl system have 4 main compoentn s
In the figure \ref{img:atari_rl}, the brain represents the agent. The Agent is our
computational system or decision making system.
The Agent in RL interacts with environment by performing actions. An action would be moving a joystick to the right.
By moving the joystick
we interact with the environment, which in this case is the true state of the Atari game engine.
After the environment receives an action, it gives back to the agent an observation
in the form of the video frame shown on the screen
and a reward signal which reflects the points scored.
\\
\paragraph{State} \label{sec:comp_rl}
Let's now abstract from our example and describe the flow of RL system more precisely.
At each time step $t$ the agent executes an action $A_t$, receives the observation $O_t$
and receives scalar reward $R_t$. The environment receives an action $A_t$,
emits observation $O_{t+1}$, emits scalar reward $R_{t+1}$. Then $t$ is incremented
after the environment's step. In RL instead of working with observations, one works
with the \emph{state} or \emph{agent state}. The agent state is the data the agent uses
to pick the next action. The state is formally a function of the
history(data that agent received so far):

\begin{align} \label{eq:rl_state}
	H_t = O_1, R_1, A_1, O_2, R_2, A_2, ..., O_t, R_t, A_t \\
	S_t^a = f(H_t)
\end{align}
where $H_t$ - is the history object at time step $t$ and
$S_t^a$ - is the agent's state at time step $t$

Because history $H_t$ can be very hard to maintain as it grows rapidly over the time,
it is very common to talk about \emph{markov state} in RL. Markov state is meant
to contain all useful information from the history as well as possess of \emph{markov property}:
\begin{equation} \label{eq:markov_property}
	P[S_{t+1} | S_t^a] = P[S_{t+1}^a | S_1^a, ..., S_t^a]
\end{equation}
where $S_t^a$ - is the agent's state at time step $t$.
It means that the state is only dependent on the present state and not on
successors states. Hence, once the state is known, we can erase the history.

\paragraph{Reward} As mentioned before $R_t$ is scalar feedback signal, which
indicates how well agent is doing at time step $t$. The job of an agent to
maximise the sum of rewards received after $t$ steps. This sum is also known as
cumulative reward.

Additionally to this an agent may possess the following components:
a \emph{policy},
a \emph{value function}, and
a \emph{model of the environment}.

\paragraph{A Policy} is an agent's behaviour function. It maps the agent's state
to actions to be taken by the agent, when the agent is in those states. Normally, the policy is
something that we want to find. Once the best policy is known, we have solved RL problem.
The Policy can be deterministic: $a = \pi (s)$
as well as stochastic: $\pi(a|s) = P[A_t = a|S_t^a = s]$.

% \paragraph{Reward function}

\paragraph{Value function} describes how good is it to be in a particular state.
% It maps a state to \emph{return} when following policy $\pi$. Return is cumulative
% reward from time step $t$ on if we follow the policy $\pi$.
The value of a state is the total amount of reward an agent can expect
to receive when following policy $\pi$, starting from that state:

\begin{equation} \label{eq:general_value_function}
	v_{\pi}(s) = \mathbb{E}_{\pi} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... | S_t^a = s]
\end{equation}
where $\mathbb{E}_{\pi}$ is the expectation of the cumulative reward from time step $t$
following policy $\pi$ given the state $s$, $\gamma$ - is a discount factor [0, 1].
 which will be explained in \autoref{subs:reward_process}\\
While the reward determines how well the current action is at given time step, value of a state
gives us more information about long term desirability of a state, that is
taking into account
the values of all possible states an agent can end up in after leaving this state.
It's crucial to understand the difference between reward and value: reward is immediate,
while the value gives us insights about the cumulative reward the agent can possibly get
from this state on.

\paragraph{Model of the environment} This represents what the environment will do next. Given
the current state $s$ and action $a$, this model defines the probability of an agent to end up in
a state $s^\prime$:
\begin{align} \label{eq:model_env}
P^a_{ss^{\prime}} = P[S_{t+1} = s_0 | S_t^a = s, A_t = a] \\
R^a_s = \mathbb{E}[R_{t+1} | S_t^a = s, A_t = a]
\end{align}
where $P$ is state transition probability matrix and $R$ is a reward give
	the probability of next state given current state and action:

\subsection{Partially Observable Environments}
One distinguishes between two type of environments in RL problems.
\emph{Fully observable environments} where an agent is capable of directly
observing the state of environment:
$O_t = S_t^a = S_t^e$ - where $S_t^e$ is environment's state,
and partially observable environment. In this work we will concentrate
on \emph{partially observable environments}.

\paragraph{Partially observable environments}
In partially observable environments, the agent's state is not equal to the environment state,
instead the agent is constructing his own representation of the environment state from
the external input(observations) that the environment provides.
Partially observable environments is a special instance of what is known in RL community
as a partially observable Markov decision process (POMDP). In our work we are constructing
the agent's state by injection the input provided by environment into RNN:
\begin{equation} \label{eq:rnn_state}
	S_t^a = \sigma(S_{t-1}^a \cdot W_s + O_t \cdot W_o)
\end{equation}
where $S_t^a$ and $S_{t-1}^a$ are agent state at time step $t$ and $t-1$
respectively, $O_t$ - is external input (in our work that is glimpse),
and $W_s$, $W_o$ - appropriate weights.
% We will investigate in POMDP more in \autoref{subs:POMDP}.

% \paragraph{Partially Observable Environments}

% TODO: tomorrow
% explain the main ideas only behind  Markov decision process
% explain a bit about temporal difference algorithm
% explain reinforce

\subsection{Markov Decision Processes(MDP)} \label{subs:reward_process}
The agent is the algorithm that we trying to build, which interacts with
the environment. \emph{Markov Decision Process(MDP)} describes this environment.
Markov Decision Process is an extension of a Markov chain and is one of the core
concepts that is used in reinforcement learning. Almost all problems in RL
can be described by using MDP. We have already defined some elements of MDP in \autoref{sec:comp_rl},
however MDP's theory gives us equations that help to solve problems.

\paragraph{Main components of MDP} The MDP is defined as using following elements:
\begin{itemize}
	\item \emph{Finite set of state}s $\mathcal{S}$ - a set of Markov states that we described in \autoref{sec:comp_rl}
	\item \emph{Finite set of actions} $\mathcal{A}$ - a set of all possible actions.
		An action $A_t = a$ \in $\mathcal{A}(S_t)$, where $\mathcal{A}(S_t)$ -
		is a set of all possible actions that can be taken in state $S_t$
	\item \emph{Reward function} $R$ - is the function which describes
		the reward based on a state and action: $R_s^a = \mathbb{E}[R_{t+1}| S_t = s, A_t = a]$
	\item \emph{State transition probability matrix} $\mathcal{P}$ give
		the probability of next state given current state and action: $P^a_{ss^{\prime}} = P[S_{t+1} = s_0 | S_t^a = s, A_t = a]$
	\item \emph{Discount factor} $\gamma$ - the discount factor determines the present value
	 	of future rewards.
\end{itemize}

Another important definition that is used in MDP known as return $G_t$. Return is nothing
more than all cumulative reward that an agent can get from time step $t$:

\begin{equation} \label{eq:return}
	G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+2} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
\end{equation}
where $R_t$ - is immediate reward at time step $t$,
and $\gamma$ - is the discount factor.

Policy in MDP is a distribution over actions given a state.
It maps a state $s$ to a probability of taking action $a$, where $a \in \mathcal{A}(S_t = s)$:
\begin{equation}
	\pi(a|s) = P[A_t = a|S_t^a = s]
\end{equation}

Policies in MDP are time independent. That is, no matter what time step $t$ it is, we have still the same
policy distribution at each time step.

\subparagraph{Discount} Most rewards in MDP are discounted by discount factor $\gamma$.
There are several reason for that. Firstly, it is mathematically convenient
to use discounts as this will prevent the return value from being exploded to infinity.
Secondly, discount give us a way to tune the model to prefer the immediate reward over
delayed or vice versa.
\paragraph{Value functions in MDP} MDP distinguishes between two types of value functions:
\emph{the state-value function} and \emph{the action-value function}.

State-value function is basically the same value function that we defined in \autoref{sec:comp_rl}:
it describes the \emph{value} of state $S_t$ when following policy $\pi$ which is expected return
starting from state $S_t$ and then following policy $\pi$. We can rewrite the equation \ref{eq:general_value_function}
using our new definition of return $G_t$:
\begin{equation} \label{eq:value_state}
	v_{\pi}(s) = \mathbb{E}_{\pi} [G_t |S_t = s]
\end{equation}
where $\mathbb{E}_{\pi}$ - is the expected value of the state $S_t$ when
following policy $\pi$.

Action-value function is defined in very similar way beside the fact that it also takes
into consideration the action taken by the agent at time step $t$. It's denoted $q_{\pi}(s, a)$
and equal to expected return starting from state $s$, taking the action $a$ and then following
policy $\pi$:

\begin{equation} \label{eq:action_value_func}
	q_{\pi}(s, a) = \mathbb{E}_{\pi} [G_t |S_t = s, A_t = a]
\end{equation}

\subparagraph{Monte Carlo methods}

This value functions can be estimated using Monte Carlo methods, which works
only for episodic environments, i.e. environment with an end state.
The idea is to let agent play in the environment by following an arbitrary policy,
sample the rewards and eventually calculate the mean of the rewards with respect to state.
Every time an agent visit state $s$, we increase the counter
of this state by 1, as well as increase total return of this state from all episodes:
\begin{align}
	N(s) = N_{last}(s) + 1 \\
	S(s) = S_{last}(s) + G_t\\
	V(s) = N_t(s) / S_t(s)
\end{align}
where $N(s)$ - is the counter an agent visited state $s$,
$G_t$ - is the return an agent got starting from state $s$,
$V(s)$ - is the estimated value of state $s$.

By the law of big numbers $V(s) \rightarrow v_{\pi}(s)$ as $N(s) \rightarrow \infty$.



\paragraph{Bellman equations} The most important reason that MDPs is used in RL problems because
it give the opportunity to use \emph{Bellman equations}. Bellman equations forms the way
of computing, optimizing and evaluating value functions $v_{\pi}(s)$ and $q_{\pi}(s, a)$.

\subparagraph{Bellman expectation equation} decomposes the value functions to represent it
in a recursive way through the value of successor state:

\begin{align*}
	v_{\pi}(s) = \mathbb{E}_{\pi} [G_t |S_t = s] \\
		= \mathbb{E}_{\pi} [\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} |S_t = s] \\
		= \mathbb{E}_{\pi} [R_{t+1} + \gamma \sum_{k=0}^{\infty} \gamma^k R_{t+k+2} |S_t = s] \\
		= \mathbb{E}_{\pi} [R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s]
\end{align*}

In a similar way, action-value function is decomposed:
\begin{align*}
	q_{\pi}(s, a) = \mathbb{E}_{\pi} [G_t |S_t = s, A_t = a] \\
		= \mathbb{E}_{\pi} [ |S_t = s, A_t = a] \\
		= \mathbb{E}_{\pi} [R_{t+1} + \gamma \sum_{k=0}^{\infty} \gamma^k R_{t+k+2} |S_t = s, A_t = a] \\
		= \mathbb{E}_{\pi} [R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s, A_t = a]
\end{align*}

If we look beneath the math, these equations formally tell us that the value of a state is equal to the
immediate reward that the agent get after leaving current state plus discounted value
of the state where the agent will end up after leaving the current state.

% \textbf{TODO:}


\subparagraph{Optimal policy}

Now that we know how to represent the value of
a state, we can concentrate on the problem that we really care about:
find the best behaviour for the MDP. That is, the policy that achieves more of reward
compared with other policies.
MDP literature give us a definition of the what is
known as optimal policy:

\blockquoute{
A policy $\pi$ is defined to be better than or equal to a policy $\pi^{\prime}$
if its expected return is greater than or equal to that of $\pi^{\prime}$ for all states.
In other words, $\pi \geq \pi^{\prime}$ if and only if $v_{\pi}(s) \geq v_{\pi^{\prime}}(s)$
for all $s \in \mathcal{S}$. There is always at least one policy that is better than or equal to all other
policies. This is an optimal policy.
} \cite{Sutton2012}

There can be more than one optimal policy, nonetheless we denote an optimal policy as
$\pi_{*}$. The goal of the any RL task is to find the optimal policy.

% \subparagraph{Optimal value function}
% We can easily find the optimal policy once we knwo
% \paragraph{Explorations, greedy policy}




% \textbf{TODO: Explaint glie monte carlo control}
% \paragraph{Monte Carlo method,} How to assert value to return, i.e. represent value.
% \textbf{TODO:} also stricti definition. LOOK up latex definition.

% \paragraph{Intuition behind value function: Lookup table}
% In order to beeing able comprehend the concept of value function let's consider
% it's representation as lookup table. Imagine environment
% CartPole which can be described following:
%
%
%
%
%
% \blockquote{
% 	A pole is attached by an un-actuated joint to a cart, which moves along a
% 	frictionless track. The system is controlled by applying a force of +1 or -1
% 	to the cart. The pendulum starts upright, and the goal is to prevent it from
% 	falling over. A reward of +1 is provided for every time step that the pole remains
% 	upright. The episode ends when the pole is more than 15 degrees from vertical, or the
% 	cart moves more than 2.4 units from the center.
% }
% We can represent state as tuple of every unit of horizontal space and
% the slope between pole and vertical. As example it can look like following:
% \begin{table}
% 	\caption{Example of state space represented as look up table}
% 	\label{}
%
% \end{table}
%
% Now that we know how to define a state space we need to represent actions. Actions
% in this example would R and L, which moves the the card 1 unit to the right
% and to the left per time step respectively. After we defined the action
% space, we just calculate the reward according to the state by using monte carlo method.
% So our lookup can look as following:
%
%
% Once we are done, can choose the policy by always choosing the state which has
% the biggest value.
% https://gym.openai.com/envs/CartPole-v0

% Least Squares Prediction
% http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/FA.pdf
% describe example as on page 35



% Table lookup is a special case of linear value function
% approximation

% Once we know how to represent the value function, It's time to think about
% possibilities to evaluate the policy as this will solve our RL problems
% talk abot optimality equaiton
% and the move to reinfoce explanaiton,



% s expected return starting from state $S_t$ and following policy $\pi$.
% Using return we can now represent our value function as the expectation of return:

% \begin{equation} \label{eq:value_return}
% 	v(s) = \mathbb{E} [G_t | S_t,]
% \end{equation}
% v(s) = E [Gt
% | St = s



\subsection{Policy-Based Reinforcement Learning}
\label{ss:pol_based_rl}
% Now, when we understand how is the environment managed, we can finally
% look at the concept of policy more accurately.
Policy-Based Reinforcement Learning is a field in RL where agent works directly
with the policy and does not necessary represents a value function or a model,
but still may compute action-value or state-value functions.\\
We'll use policy-based approaches in this work as those methods have better
convergent properties more efficient for high-dimensional action space.
\cite{DBLP:journals/corr/MnihHGK14}
%TODO: maybe explain why policy based?


% as we well as assert value to them to improve the policy by using
% f.i. $\epsilon$-greedy policy improvement. We can do it using lookup table to save
% our states
% but
Firstly, we need to find a way to represent the policy. With the near continuous
action space to store
all states will require a lot of memory. Therefore for problems with large
state space it's recommended to use function approximation methods.

Using function approximators such as neural network will allow to
estimate policy function:

% \begin{align} \label{eq:value_appr_func}
% 	\hat{v}(s, W) \approx v_{\pi}(s) \\
% 	\hat{q}(s, a, W) \approx q_{\pi}(s, a)
% \end{align}
\begin{align} \label{eq:policy_appr_func}
	\pi_{\theta}(s) = \mathbb{P}[a|s, \theta]
\end{align}
where $\mathbb{P}[a|s, \theta]$ - is the probability of taking action $a$
given state $s$.

$\pi_{\theta}$ is the policy with parameters $\theta$. We can represent our policy
with the help of different function approximators. The most common function
approximator used in RL is neural network that was introduced in \autoref{sec:neural_networks}.
So $\theta$ can represent parameters of neural network, although any other function approximator
can be used here such as decision tree or nearest neighbour approximator.

Our goal is to find the optimal policy with the parameters $\theta$
directly from the agent's experience.

% the dynamics of environment.
% That is, we let an agent to play with the environment
% and the agent needs to to adjust the parameters $\theta$ relying only
% on reward signal from agent's experience so to get
% more reward from the environment.

\subparagraph{Note:}
The representation in \ref{eq:policy_appr_func} will even allow to generalise
the policy from seen states to unseen.

%
% As we will concentrate in this work mostly on policy
% In a similar way, we can also estimate policy function:



% \textbf{TODO: polcy approximation}


% \hat{v}(s, W) \approx v_{\pi}(s) \\
% \hat{q}(s, a, W) \approx q_{\pi}(s, a)

% Let's make acquaintance with the notation we are going to use. ???


\subparagraph{Cost} Now we have a way to initialise our policy with any parameters $\theta$.
% The goal of policy-based approaches is to find the best parameters $\theta$ for
% a policy so the learned policy will be close to the optimal policy $/pi_{*}$.
However in order to estimate the policy, we also need to find a way
to measure it's quality, i.e. define the cost function $J$.
One way to define the cost function is to assign it
to the value of the start state:

\begin{equation}
	J(\theta) = V^{\pi_{\theta}}(s_1) = \mathbb{E}_{\pi_{\theta}} [v_1]
\end{equation}
where $s_1$ - is the first state in an episode, $v_1$ - is the value
of the first state.



To remind you, the start value is the return that an agent will get from start state
following policy $\pi_{\theta}$. This will work only for environment with an end state.


\subparagraph{Policy} Our agent needs to take action while going through
the environment.
For continuous state space it's common to
use Gaussian distribution with fixed variance $\sigma^2$ and parametrized mean:
\begin{align} \label{eq:param_mean}
	\mu = \phi(s)^{T} \theta \\
	a \sim \mathcal{N}(\mu(s), \sigma^2)
\end{align}
where $\phi(s)^{T}$ - is a feature vector of state $s$. \\
Feature vector normally represents the availability of certain features in
a particular state $s$. \\
For example, the first entry in the feature can determine
whether a robot is close to a wall. In state $s_1$
where the robot actually has a wall a close to him
the first entry should be then equal to 1 or close to 1.
In contrast to state $s_2$ where
robot has no wall close by, this entry in the feature vector
should be 0 or close to 0.
The feature vector might have hundreds of entry features like this.


% close
% There is a variety of ways to defined the policy, but one that we will use in this
% work knows as Gaussian policy.

% The easiest way of measuring it would be to represent the object function
% as the expected value of all states that an
% agent will placed when following this policy
%
% \begin{equation}
% 	J(\theta) = \mathbb{E}_{\pi{\theta}} [Q^{\pi_{\theta}}(s, a)]
% \end{equation}



% In our problem, observations will come from glimpses to RNN which we will use
% as a state for our agent:
% \begin{equation} \label{eq:rnn_state}
% 	S_t^a = \sigma(S_{t-1}^a \cdot W_s + O_t \cdot W_o)	\tag{\ref{eq:rnn_state} revisited}
% \end{equation}
% where $S_t^a$ and $S_{t-1}^a$ are agent state at time step $t$ and $t-1$
% respectively, $O_t$ - is external input (in our work that is glimpse),
% and $W_s$, $W_o$ - appropriate weights.



% policy can be chosen as gaussian distribution with mean  = v(s) * \thetha

\paragraph{Policy gradient theorem}
Now that we know the objective function, we can use gradient ascent
algorithm to maximise it. One normally uses Policy gradient
algorithm to estimate the gradient:

\begin{equation} \label{eq:reinforce}
	\Delta_{\theta} J(\theta) = \mathbb{E}_{\pi_{\theta}}
		[\Delta_{\theta} \log(
		\pi_{\theta}(s, a) )  Q^{\pi_{\theta}}(s, a)]
\end{equation}

where $Q_{\pi_{\theta}}(s, a)$ - is the action-value function.
% for example, Monte Carlo method described earlier.

\paragraph{Reinforce rule}
Using return $v_t$ as an unbiased sample from $Q^{\pi_{\theta}}(s, a)$
in \ref{eq:reinforce} we can derive the REINFORCE learning rule:

\begin{equation} \label{eq:reinfforce_rule}
	\Delta \theta_t = \alpha \Delta_{\theta} \log \pi_{\theta}(s_t, a_t) v_t
\end{equation}


Return $v_t$ can be estimated by using Monte Carlo method.
\subparagraph{Reducing variance} The equation \ref{eq:reinfforce_rule} still has
a high variance. We can try to reduce it by subtracting the function knows as
baseline $B(s)$. If we subtract this function, it won't change the expectation
as:

\begin{equation} \label{eq:bas_0}
	\mathbb{E}_{\pi_{\Delta}} [\Delta_{\Delta} \log \pi_{\Delta}(s, a) B(s)] = 0
\end{equation}

Therefore we can rewrite the equation \ref{eq:reinforce} in the following
way:
\begin{equation}
	\Delta \theta_t = \alpha \Delta_{\theta} \log \pi_{\theta}(s_t, a_t) (Q_{\pi_{\theta}}(s, a) - B(s))
\end{equation}
The common practice in RL to choose baseline function $B(s)$ equal to value function:
 $B(s) = V^{\pi_{\theta}}(s)$. The term $(Q^{\pi_{\theta}}(s, a) -  V^{\pi_{\theta}}(s))$
 is known as advantage function. Advantage function represents how the value
 of being in state $s$ and taking an action $a$, is better than value of being in state $s$
 summarizing over all possible actions that can be taken in state $s$.
%
% As suggested by in \ref{definition_value},
% which is unbiased sample from value function in .
%
% and then actual sequential of steps from.
% The whole process can simulated as following:

% \begin{orderedlisting}
% 	\item Initialise $\theta$ randomly
% 	\item do a $t$ of episodes
% 	and etc.
% \end{orderedlisting}

% The pseudo code
% for the REINFORCE rule will be like following:
%
% Or maybe just explain it with the words, lile:

% \begin{lstlisting}
% 	function REINFORCE
% 		Initialise θ arbitrarily
% 		for each episode {s1, a1,r2, ...,sT−1, aT−1,rT } ∼ πθ do
% 		for t = 1 to T − 1 do
% 		θ ← θ + α∇θ log πθ(st
% 		, at)vt
% 		end for
% 		end for
% 		return θ
% 		end function
% \end{lstlisting}
% the way you actually computing it.


% \subsection{POMDP} \label{subs:POMDP}




% write about main components
% * state
% * value
% * return and reward
% *
% about markov decision process
% about types of environments
%

% MOMDP
% Reinforce rule
% reducing variance by using advantage function
