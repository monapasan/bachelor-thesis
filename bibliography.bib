@MASTERSTHESIS {,
    author = "Oleg Yarin",
    title  = "Development and Evaluation of a Visual Attention Model with Python and Tensorflow",
    school = "HTW Berlin",
    year   = "2017"
}


@article{DBLP:journals/corr/MnihHGK14,
  author    = {Volodymyr Mnih and
               Nicolas Heess and
               Alex Graves and
               Koray Kavukcuoglu},
  title     = {Recurrent Models of Visual Attention},
  journal   = {CoRR},
  volume    = {abs/1406.6247},
  year      = {2014},
  url       = {http://arxiv.org/abs/1406.6247},
  timestamp = {Wed, 07 Jun 2017 14:41:58 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/MnihHGK14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{DBLP:journals/corr/BaMK14,
  author    = {Jimmy Ba and
               Volodymyr Mnih and
               Koray Kavukcuoglu},
  title     = {Multiple Object Recognition with Visual Attention},
  journal   = {CoRR},
  volume    = {abs/1412.7755},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.7755},
  timestamp = {Wed, 07 Jun 2017 14:43:09 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BaMK14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{DBLP:journals/corr/Goldsborough16,
  author    = {Peter Goldsborough},
  title     = {A Tour of TensorFlow},
  journal   = {CoRR},
  volume    = {abs/1610.01178},
  year      = {2016},
  url       = {http://arxiv.org/abs/1610.01178},
  timestamp = {Wed, 07 Jun 2017 14:41:13 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/Goldsborough16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@misc{CAMEL,
title = {{CAMELYON17 - Home}},
url = {https://camelyon17.grand-challenge.org/},
urldate = {2017-05-12}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@book{Bishop2006,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bishop, Christopher M},
booktitle = {Springer Science+Business Media},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/olegya/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - full-text.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Pattern Recognition and Machine Learning}},
volume = {53},
year = {2006}
}

@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:Users/olegya/Library/Application Support/Mendeley Desktop/Downloaded/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks},
year = {2012}
}

@Book{ Kriesel2007NeuralNetworks,
   author = { David Kriesel },
   title =  { A Brief Introduction to Neural Networks },
   year =   { 2007 },
   url =   { available at http://www.dkriesel.com }
}

@misc{Nielsen2015,
author = {Nielsen, Michael A.},
publisher = {Determination Press},
title = {{Neural Networks and Deep Learning}},
url = {http://neuralnetworksanddeeplearning.com/},
year = {2015}
}

@book{VanDerSmagt1996,
author = {{Van Der Smagt}, Patrick and Krose, Ben and Krr, Ben and {An Der Smagt}, Patrick V},
edition = {8},
file = {:Users/olegya/Library/Application Support/Mendeley Desktop/Downloaded/Van Der Smagt et al. - 1996 - An introduction to Neural Networks.pdf:pdf},
publisher = {University of Amsterdam},
title = {{An introduction to Neural Networks}},
url = {http://www.fwi.uva.nl/research/neuro/ http://www.fwi.uva.nl/research/neuro/},
year = {1996}
}

@article{Bishop1995,
abstract = {This book provides a solid statistical foundation for neural networks from a pattern recognition perspective. The focus is on the types of neural nets that are most widely used in practical applications, such as the multi-layer perceptron and radial basis function networks. Rather than trying to cover many different types of neural networks, Bishop thoroughly covers topics such as density estimation, error functions, parameter optimization algorithms, data pre-processing, and Bayesian methods. All topics are organized well and all mathematical foundations are explained before being applied to neural networks. The text is suitable for a graduate or advanced undergraduate level course on neural networks or for practitioners interested in applying neural networks to real-world problems. The reader is assumed to have the level of math knowledge necessary for an undergraduate science degree. This is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition. After introducing the basic concepts, the book examines techniques for modelling probability density functions and the properties and merits of the multi-layer perceptron and radial basis function network models. Also covered are various forms of error functions, principal algorithms for error function minimalization, learning and generalization in neural networks, and Bayesian techniques and their applications. Designed as a text, with over 100 exercises, this fully up-to-date work will benefit anyone involved in the fields of neural computation and pattern recognition.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {Bishop, C M},
doi = {10.2307/2965437},
eprint = {0-387-31073-8},
file = {:Users/olegya/Library/Application Support/Mendeley Desktop/Downloaded/Bishop - Unknown - Neural Networks for Pattern Recognition.pdf:pdf},
isbn = {0198538642},
issn = {01621459},
journal = {Journal of the American Statistical Association},
pages = {482},
pmid = {1144972},
title = {{Neural networks for pattern recognition}},
url = {http://cs.du.edu/{~}mitchell/mario{\_}books/Neural{\_}Networks{\_}for{\_}Pattern{\_}Recognition{\_}-{\_}Christopher{\_}Bishop.pdf},
volume = {92},
year = {1995}
}

@book{rosenblatt1962principles,
  title={Principles of neurodynamics: perceptrons and the theory of brain mechanisms},
  author={Rosenblatt, F.},
  lccn={62012882},
  series={Report (Cornell Aeronautical Laboratory)},
  url={https://books.google.ca/books?id=7FhRAAAAMAAJ},
  year={1962},
  publisher={Spartan Books}
}

@misc{KarpathyAndrej2016,
author = {{Karpathy Andrej}},
booktitle = {Standord University},
title = {{CS231n Convolutional Neural Networks for Visual Recognition}},
url = {http://cs231n.github.io/optimization-1/},
urldate = {2017-06-04},
year = {2016}
}
\

@article{Rumelhart1986,
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
issn = {0028-0836},
journal = {Nature},
month = {oct},
number = {6088},
pages = {533--536},
publisher = {Nature Publishing Group},
title = {{Learning representations by back-propagating errors}},
url = {http://www.nature.com/doifinder/10.1038/323533a0},
volume = {323},
year = {1986}
}

@misc{ColahChristopher2015,
author = {{Colah Christopher}},
title = {{Understanding LSTM Networks -- colah's blog}},
url = {http://colah.github.io/posts/2015-08-Understanding-LSTMs/},
urldate = {2017-06-04},
year = {2015}
}

@phdthesis{Hochreiter1991,
abstract = {Ich versichere, da{\ss} ich diese Diplomarbeit selbst{\"{a}}ndig verfa{\ss}t und keine anderen als die ange-geben Quellen und Hilfsmittel benutzt habe.},
author = {Hochreiter, Josef},
booktitle = {Master's thesis, Institut fur Informatik, Technische Universitat, Munchen},
pages = {1--71},
title = {{Untersuchungen zu dynamischen neuronalen Netzen}},
url = {http://people.idsia.ch/{~}juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Untersuchungen+zu+dynamischen+neuronalen+Netzen{\#}0},
year = {1991}
}

@article{Hochreiter:1997:LSM:1246443.1246450,
 author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
 title = {Long Short-Term Memory},
 journal = {Neural Comput.},
 issue_date = {November 15, 1997},
 volume = {9},
 number = {8},
 month = nov,
 year = {1997},
 issn = {0899-7667},
 pages = {1735--1780},
 numpages = {46},
 url = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
 doi = {10.1162/neco.1997.9.8.1735},
 acmid = {1246450},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}
@article{Elman1990,
abstract = {[PDF]},
author = {Elman, Jeffrey L},
doi = {10.1207/s15516709cog1402_1},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive science},
number = {2},
pages = {179--211},
pmid = {19563812},
title = {{Finding structure in time}},
url = {https://pdfs.semanticscholar.org/4eb9/43bf999ce49e5ebb629d7d0ffee44becff94.pdf},
volume = {14},
year = {1990}
}

@inproceedings{werbos:bptt,
  added-at = {2008-03-11T14:52:34.000+0100},
  author = {Werbos, P.},
  biburl = {https://www.bibsonomy.org/bibtex/2062450f0f2629f8746f7c0e54922b0cc/idsia},
  booktitle = {Proceedings of IEEE},
  citeulike-article-id = {2380325},
  interhash = {00557ce797fc197789230a80f431dc10},
  intrahash = {062450f0f2629f8746f7c0e54922b0cc},
  keywords = {inaki},
  number = 10,
  pages = {1550--1560},
  priority = {2},
  timestamp = {2008-03-11T14:56:15.000+0100},
  title = {Backpropagation through time: what does it do and how to do it},
  volume = 78,
  year = 1990
}

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@article{Sutton2012,
author = {Sutton, Richard S and Barto, Andrew G},
file = {:Users/olegya/Library/Application Support/Mendeley Desktop/Downloaded/Sutton, Barto - Unknown - Reinforcement Learning An Introduction.pdf:pdf},
title = {{Reinforcement Learning: An Introduction}},
url = {http://people.inf.elte.hu/lorincz/Files/RL{\_}2006/SuttonBook.pdf},
year = {2012}
}


@article{Larochelle2010,
abstract = {We describe a model based on a Boltzmann machine with third - order connections that can learn how to accumulate information about a shape over several fixations. The model uses a retina that only has enough high resolution pixels to cover a small area of the image, so it must ...},
author = {Larochelle, Hugo and Hinton, Geoffrey},
doi = {10.1371/journal.pone.0003290},
file = {:Users/olegya/Library/Application Support/Mendeley Desktop/Downloaded/Larochelle, Hinton - 2010 - Learning to combine foveal glimpses with a third-order Boltzmann machine.pdf:pdf},
isbn = {9781617823800},
issn = {1932-6203},
journal = {Nips-2010},
pages = {1243--1251},
pmid = {18820727},
title = {{Learning to combine foveal glimpses with a third-order Boltzmann machine}},
url = {papers2://publication/uuid/4E7CE0E0-C8F9-49B9-9B45-051B57B8DDF5{\%}5Cnpapers2://publication/uuid/6610B2C4-A06C-4AD7-9041-5557B73F415D},
year = {2010}
}


@book{Martin:2008:CCH:1388398,
 author = {Martin, Robert C.},
 title = {Clean Code: A Handbook of Agile Software Craftsmanship},
 year = {2008},
 isbn = {0132350882, 9780132350884},
 edition = {1},
 publisher = {Prentice Hall PTR},
 address = {Upper Saddle River, NJ, USA},
}


@book{Pressman:2001:SEP:572512,
 author = {Pressman, Roger S.},
 title = {Software Engineering: A Practitioner's Approach},
 year = {2001},
 isbn = {0072496681},
 edition = {5th},
 publisher = {McGraw-Hill Higher Education},
}


@book{McConnell:2004:CCS:1096143,
 author = {McConnell, Steve},
 title = {Code Complete, Second Edition},
 year = {2004},
 isbn = {0735619670, 9780735619678},
 publisher = {Microsoft Press},
 address = {Redmond, WA, USA},
}

@misc{Rossum,
author = {van Rossum, Guido and Warsaw, Barry and Coghlan, Nick},
booktitle = {Python.org},
keywords = {Python programming language object oriented web fr},
pages = {1--25},
title = {{PEP 0008: style guide for python code}},
url = {https://www.python.org/dev/peps/pep-0008/},
urldate = {2017-06-13},
year = {2001}
}

@misc{Reitz,
author = {Reitz, Kenneth},
title = {{The Hitchhiker's Guide to Python! — The Hitchhiker's Guide to Python}},
url = {http://docs.python-guide.org/en/latest/},
urldate = {2017-06-13}
}

@misc{Goodger2001,
abstract = {This PEP documents the semantics and conventions associated with Python docstrings.},
author = {Goodger, David and van Rossum, Guido},
keywords = {Python programming language object oriented web fr},
title = {{PEP 257 -- Docstring Conventions | Python.org}},
url = {https://www.python.org/dev/peps/pep-0257/},
urldate = {2017-06-13},
year = {2001}
}


@article{Goldsborough,
abstract = {— Deep learning is a branch of artificial intelligence employing deep neural network architectures that has signifi-cantly advanced the state-of-the-art in computer vision, speech recognition, natural language processing and other domains. In November 2015, Google released TensorFlow, an open source deep learning software library for defining, training and deploying machine learning models. In this paper, we review TensorFlow and put it in context of modern deep learning concepts and software. We discuss its basic computational paradigms and distributed execution model, its programming interface as well as accompanying visualization toolkits. We then compare Ten-sorFlow to alternative libraries such as Theano, Torch or Caffe on a qualitative as well as quantitative basis and finally comment on observed use-cases of TensorFlow in academia and industry.},
author = {Goldsborough, Peter},
file = {:Users/olegya/Library/Application Support/Mendeley Desktop/Downloaded/Goldsborough - Unknown - A Tour of TensorFlow Proseminar Data Mining.pdf:pdf},
keywords = {Distributed Computing,Index Terms— Artificial Intelligence,Machine Learning,Neu-ral Networks,Open source software,Software packages},
title = {{A Tour of TensorFlow Proseminar Data Mining}},
url = {https://arxiv.org/pdf/1610.01178.pdf}
}


@misc{,
title = {{TensorFlow}},
url = {https://www.tensorflow.org/},
urldate = {2017-05-12}
}


@article{DBLP:journals/corr/KingmaB14,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  journal   = {CoRR},
  volume    = {abs/1412.6980},
  year      = {2014},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Wed, 07 Jun 2017 14:40:52 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/KingmaB14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{Pascanu2012,
abstract = {There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
archivePrefix = {arXiv},
arxivId = {1211.5063},
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
doi = {10.1109/72.279181},
eprint = {1211.5063},
file = {:Users/olegya/Library/Application Support/Mendeley Desktop/Downloaded/Pascanu, Mikolov, Bengio - 2012 - On the difficulty of training Recurrent Neural Networks.pdf:pdf},
isbn = {08997667 (ISSN)},
issn = {1045-9227},
month = {nov},
pmid = {18267787},
title = {{On the difficulty of training Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1211.5063},
year = {2012}
}

@misc{tensorflow2015-whitepaper,
title={ {TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
url={http://tensorflow.org/},
note={Software available from tensorflow.org},
author={
    Mart\'{\i}n~Abadi and
    Ashish~Agarwal and
    Paul~Barham and
    Eugene~Brevdo and
    Zhifeng~Chen and
    Craig~Citro and
    Greg~S.~Corrado and
    Andy~Davis and
    Jeffrey~Dean and
    Matthieu~Devin and
    Sanjay~Ghemawat and
    Ian~Goodfellow and
    Andrew~Harp and
    Geoffrey~Irving and
    Michael~Isard and
    Yangqing Jia and
    Rafal~Jozefowicz and
    Lukasz~Kaiser and
    Manjunath~Kudlur and
    Josh~Levenberg and
    Dan~Man\'{e} and
    Rajat~Monga and
    Sherry~Moore and
    Derek~Murray and
    Chris~Olah and
    Mike~Schuster and
    Jonathon~Shlens and
    Benoit~Steiner and
    Ilya~Sutskever and
    Kunal~Talwar and
    Paul~Tucker and
    Vincent~Vanhoucke and
    Vijay~Vasudevan and
    Fernanda~Vi\'{e}gas and
    Oriol~Vinyals and
    Pete~Warden and
    Martin~Wattenberg and
    Martin~Wicke and
    Yuan~Yu and
    Xiaoqiang~Zheng},
  year={2015},
}

@misc{TfWeb,
title = {{TensorFlow Style Guide  |  TensorFlow}},
url = {https://www.tensorflow.org/community/style{\_}guide},
urldate = {2017-06-15}
}

@misc{Goodger2001,
abstract = {This PEP documents the semantics and conventions associated with Python docstrings.},
author = {Goodger, David and van Rossum, Guido},
keywords = {Python programming language object oriented web fr},
title = {{PEP 257 -- Docstring Conventions | Python.org}},
url = {https://www.python.org/dev/peps/pep-0257/},
urldate = {2017-06-13},
year = {2001}
}

@misc{Reitz,
author = {Reitz, Kenneth},
title = {{The Hitchhiker's Guide to Python! — The Hitchhiker's Guide to Python}},
url = {http://docs.python-guide.org/en/latest/},
urldate = {2017-06-13}
}

@misc{Rossum,
author = {van Rossum, Guido and Warsaw, Barry and Coghlan, Nick},
booktitle = {Python.org},
keywords = {Python programming language object oriented web fr},
pages = {1--25},
title = {{PEP 0008: style guide for python code}},
url = {https://www.python.org/dev/peps/pep-0008/},
urldate = {2017-06-13},
year = {2001}
}

@misc{Reitz,
author = {Reitz, Kenneth},
title = {{The Hitchhiker's Guide to Python! — The Hitchhiker's Guide to Python}},
url = {http://docs.python-guide.org/en/latest/},
urldate = {2017-06-13}
}

@book{beck1997smalltalk,
  title={Smalltalk Best Practice Patterns. Volume 1: Coding},
  author={Beck, Kent},
  year={1997},
  publisher={Prentice Hall, Englewood Cliffs, NJ}
}

@book{1999:RID:311424,
 title = {Refactoring: Improving the Design of Existing Code},
 year = {1999},
 isbn = {0-201-48567-2},
 publisher = {Addison-Wesley Longman Publishing Co., Inc.},
 address = {Boston, MA, USA},
key = {{$\!\!$}} ,
}

@book{martin2003agile,
  title={Agile Software Development: Principles, Patterns, and Practices},
  author={Martin, R.C.},
  isbn={9780135974445},
  lccn={2003270283},
  series={Alan Apt series},
  url={https://books.google.nl/books?id=0HYhAQAAIAAJ},
  year={2003},
  publisher={Pearson Education}
}

@misc{LeCun2010,
author = {LeCun, Yann and Cortes, Corinna and Burges, Christopher J.C.},
title = {{MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burges}},
url = {http://yann.lecun.com/exdb/mnist/ http://yann.lecun.com/exdb/mnist/index.html},
urldate = {2017-04-12},
year = {2010}
}

@book{Gamma:1995:DPE:186897,
 author = {Gamma, Erich and Helm, Richard and Johnson, Ralph and Vlissides, John},
 title = {Design Patterns: Elements of Reusable Object-oriented Software},
 year = {1995},
 isbn = {0-201-63361-2},
 publisher = {Addison-Wesley Longman Publishing Co., Inc.},
 address = {Boston, MA, USA},
}

@article{Eckel2017,
author = {Eckel, Bruce},
pages = {1--301},
title = {{Python 3 Patterns, Recipes and Idioms}},
url = {https://media.readthedocs.org/pdf/python-3-patterns-idioms-test/latest/python-3-patterns-idioms-test.pdf},
year = {2008}
}
